import re
text = "Hello, world. This, is a test."
result = re.split(r'(\s)', text)
print(result)

//

text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)
//

import torch

model_name = 'gpt2-large'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

def generate_text(prompt, max_length=100, num_return_sequences=1, temperature=0.7):
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, padding=True)

    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_length=max_length,
            num_return_sequences=num_return_sequences,
            temperature=temperature,
            pad_token_id=tokenizer.eos_token_id
        )

    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

prompt = "In the realm of artificial intelligence, one of the most exciting advancements is"
generated_texts = generate_text(prompt, max_length=150, num_return_sequences=3)

for idx, text in enumerate(generated_texts):
    print(f"Generated Text {idx + 1}:\n{text}\n")

def process_large_input(text, max_length=512):
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]
    outputs = []

    for chunk in chunks:
        input_ids = torch.tensor([chunk])
        with torch.no_grad():
            chunk_output = model.generate(
                input_ids,
                max_length=max_length,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        outputs.append(tokenizer.decode(chunk_output[0], skip_special_tokens=True))

    return ' '.join(outputs)

large_text = "A very long text that exceeds the model's input limits..."
processed_text = process_large_input(large_text)
print(processed_text)

import torch.nn as nn
class SelfAttention_v1(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))

    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        attn_scores = queries @ keys.T # omega
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec
